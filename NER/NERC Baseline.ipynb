{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NERC Project for Recognizing and Classifying Drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence examples\n",
    "sent_1 = \"Activation of an effector immediate-early gene arc by methamphetamine\"\n",
    "sent_2 = \"In situations in which concurrent therapy is necessary, careful patient monitoring is essential.\"\n",
    "sent_3 = \"Phenothiazines and 3-butyrophenones may reduce or reverse the depressor effect of epinephrine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Phenothiazines', 'and', '3-butyrophenones', 'may', 'reduce', 'or', 'reverse', 'the', 'depressor', 'effect', 'of', 'epinephrine', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize word\n",
    "tokenized_sent_1 = word_tokenize(sent_1)\n",
    "tokenized_sent_2 = word_tokenize(sent_2)\n",
    "tokenized_sent_3 = word_tokenize(sent_3)\n",
    "print(tokenized_sent_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OffsetFrom: 0\n",
      "OffsetTo: 13\n",
      "\n",
      "OffsetFrom: 3\n",
      "OffsetTo: 12\n"
     ]
    }
   ],
   "source": [
    "# Use the .find() method to find offset and end position\n",
    "print(\"OffsetFrom: \" + str(sent_3.find(tokenized_sent_3[0]))) # offset\n",
    "print(\"OffsetTo: \" + str(sent_3.find(tokenized_sent_3[0]) + len(tokenized_sent_3[0]) - 1)) # end\n",
    "print(\"\")\n",
    "print(\"OffsetFrom: \" + str(sent_2.find(tokenized_sent_2[1]))) # offset\n",
    "print(\"OffsetTo: \" + str(sent_2.find(tokenized_sent_2[1]) + len(tokenized_sent_2[1]) - 1)) # end\n",
    "\n",
    "# From this results generate desired output: list of tuples (word, offsetFrom, offsetTo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['Phenothiazines', 'and', '3-butyrophenones', 'may', 'reduce', 'or', 'reverse', 'the', 'depressor', 'effect', 'of', 'epinephrine', '.']\n",
      "Filterd Sentence: ['Phenothiazines', 'may', 'reduce', 'reverse', 'depressor', 'effect', 'epinephrine']\n"
     ]
    }
   ],
   "source": [
    "# Removing Stopwords and Punctuations to Reduce Workload\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_sent = []\n",
    "\n",
    "for w in tokenized_sent_3:\n",
    "    if (w not in stop_words) & (w.isalpha()):\n",
    "        filtered_sent.append(w)\n",
    "\n",
    "print(\"Tokenized Sentence:\",tokenized_sent_3)\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    Given a sentence , calls nltk.tokenize to split it in tokens, and adds to each token its start / end offset \n",
    "    in the original sentence .\n",
    "    Input - s: string containing the text for one sentence\n",
    "    Output - Returns a list of tuples (word , offsetFrom , offsetTo )'''\n",
    "\n",
    "    token_list = []\n",
    "    tokens = word_tokenize(s)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    for t in tokens:\n",
    "        if (t in stop_words) | (t.isdigit()): # reduce workload\n",
    "            continue\n",
    "        else:\n",
    "            offsetFrom = s.find(t)\n",
    "            offsetTo = offsetFrom + len(t) - 1\n",
    "            token_list.append((t, offsetFrom, offsetTo))\n",
    "            \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Phenothiazines', 0, 13), ('3-butyrophenones', 19, 34), ('may', 36, 38), ('reduce', 40, 45), ('reverse', 50, 56), ('depressor', 62, 70), ('effect', 72, 77), ('epinephrine', 82, 92), ('.', 93, 93)]\n"
     ]
    }
   ],
   "source": [
    "# \"Activation of an effector immediate-early gene arc by methamphetamine\"\n",
    "sent = tokenize(sent_3)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources: Reading Drug DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using drug database\n",
    "resource_path = \"/Users/mponsclo/Downloads/labAHLT/resources/HSDB.txt\"\n",
    "#resource_path =\"../labAHLT/resources/HSDB.txt\"\n",
    "drug_set = set()\n",
    "with open(resource_path, 'r') as resource_file:\n",
    "    drugs = resource_file.readlines()\n",
    "    drug_set = set([d[:-1].lower() for d in drugs])\n",
    "    #print(drug_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_type_classifier(word):\n",
    "        \n",
    "    threes = [\"nol\", \"lol\", \"hol\", \"lam\", \"pam\"]\n",
    "    fours = [\"arin\", \"oxin\", \"toin\",\"pine\", \"tine\", \"bital\", \"inol\", \"pram\"]\n",
    "    fives = [\"azole\", \"idine\", \"orine\", \"mycin\", \"hrine\", \"exate\", \"amine\", \"emide\"]\n",
    "    \n",
    "    drug_n = [\"PCP\", \"18-MC\", \"methyl\", \"phenyl\", \"tokin\", \"fluo\", \"ethyl\"]\n",
    "\n",
    "    groups = [\"depressants\", \"steroid\", \"ceptives\", \"urates\", \"amines\", \"azines\", \"phenones\", \n",
    "              \"inhib\", \"coagul\", \"block\", \"acids\", \"agent\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"+\",\"-\"]\n",
    "    \n",
    "    if word.isupper() & (len(word) >= 4): \n",
    "        return True, \"brand\"  \n",
    "    elif (word[-3:] in threes) | (word[-4:] in fours) | (word[-5:] in fives):\n",
    "        return True, \"drug\"\n",
    "    elif (True in [t in word for t in drug_n]) | (word.isupper() & (len(word) < 4 & len(word) >= 2)): \n",
    "        return True, \"drug_n\"\n",
    "    elif (True in [t in word for t in groups]) | ((word[-1:] == \"s\") & (word[-2].isupper())): \n",
    "        return True, \"group\"\n",
    "    elif word in drug_set:        # Drug Database Checking --> Must be the first rule\n",
    "        return True, \"drug\"\n",
    "    else: \n",
    "        return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'group')\n",
      "(True, 'drug')\n",
      "(True, 'drug')\n",
      "(True, 'brand')\n",
      "(True, 'group')\n",
      "(True, 'group')\n",
      "(True, 'drug_n')\n"
     ]
    }
   ],
   "source": [
    "print(token_type_classifier(\"NSAIDs\"))\n",
    "print(token_type_classifier(\"cimitidine\"))\n",
    "print(token_type_classifier(\"clozapine\"))\n",
    "print(token_type_classifier(\"TAXOL\"))\n",
    "print(token_type_classifier(\"antacids\"))\n",
    "print(token_type_classifier(\"3-HydroxiButanil\"))\n",
    "print(token_type_classifier(\"3-methyl-4-aspartate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(s):\n",
    "    ''' Given a tokenized sentence , identify which tokens (or groups of consecutive tokens) are drugs\n",
    "    Input - s: A tokenized sentence ( list of triples (word , offsetFrom , offsetTo ) )\n",
    "    Output - A list of entities. Each entity is a dictionary with the keys 'name ', ' offset ', and 'type '''\n",
    "\n",
    "\n",
    "    output = []\n",
    "    for t in s:\n",
    "        tokenText = t[0] # get the only the text from (text, offsetFrom, offsetTo)\n",
    "        (is_brand_drug_group, type_text) = token_type_classifier(tokenText)\n",
    "        \n",
    "        if is_brand_drug_group:\n",
    "            offsetFrom = t[1]\n",
    "            offsetTo = t[2]\n",
    "            entity = {\"name\" : tokenText,\n",
    "                     \"offset\" : str(offsetFrom) + \"-\" + str(offsetTo), \n",
    "                     \"type\" : type_text}\n",
    "            output.append(entity)\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID|0-13|Phenothiazines|group\n",
      "ID|19-32|butyrophenones|group\n",
      "ID|80-90|epinephrine|drug\n"
     ]
    }
   ],
   "source": [
    "entity = extract_entities(sent)\n",
    "for e in entity:\n",
    "    print(\"ID\" + \"|\" + e[\"offset\"] + \"|\" + e[\"name\"] + \"|\" + e[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/Users/mponsclo/Downloads/labAHLT/data/train\"\n",
    "#datadir = \"../labAHLT/data/train\"\n",
    "def main(datadir): #, outfile):\n",
    "    '''datadir - directory with XML files\n",
    "       outfile - name for the outputfile'''\n",
    "    \n",
    "        # process each file in directory\n",
    "    for f in listdir(datadir):\n",
    "        try: \n",
    "            # parse XML file, obtaining a DOM tree\n",
    "            tree = parse(datadir + \"/\" + f)\n",
    "            # process each senetence in the file\n",
    "            sentences = tree.getElementsByTagName(\"sentence\")\n",
    "            for s in sentences:\n",
    "                    sid = s.attributes[\"id\"].value        # get sentence id\n",
    "                    stext = s.attributes[\"text\"].value    # get sentence text\n",
    "                    # tokenize text\n",
    "                    tokens = tokenize(stext)\n",
    "                    # extract entities from tokenized sentence text\n",
    "                    entities = extract_entities(tokens)\n",
    "\n",
    "                    # print sentence entities in format requested for evaluation\n",
    "                    for e in entities:\n",
    "                        print(sid + \" | \" + e[\"offset\"] + \" | \" + e[\"name\"] + \" |\" + e[\"type\"]) #, file = outfile)\n",
    "        except:\n",
    "            pass\n",
    "        # print performance score\n",
    "        #evaluator.evaluate(\"NER\", datadir, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = main(datadir)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the process of iteration through XML files in the folder\n",
    "datadir = \"/Users/mponsclo/Downloads/labAHLT/data/train\"\n",
    "for f in listdir(datadir):\n",
    "    filename = datadir + \"/\" + f\n",
    "    #print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the dataframe\n",
    "tree = ET.parse(\"/Users/mponsclo/Downloads/labAHLT/data/train/Estramustine_ddi.xml\")\n",
    "root = tree.getroot()\n",
    "    \n",
    "for elem in root:\n",
    "    for subelem in elem.findall('entity'):    \n",
    "        # if we know the name of the attribute, access it directly\n",
    "        # aux_dict = {\"text\":subelem.get('text'), \"type\":subelem.get('type')} # as a dict\n",
    "        \n",
    "        aux_df = pd.DataFrame({'Name': subelem.get('text'), 'Type': subelem.get('type')}, index=[0])\n",
    "        df = df.append(aux_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- If-elif-else Tests ----------\n",
    "threes = [\"nol\", \"lol\", \"hol\", \"lam\", \"pam\"]\n",
    "fours = [\"arin\", \"oxin\", \"toin\",\"pine\", \"tine\", \"bital\", \"inol\", \"pram\"]\n",
    "fives = [\"azole\", \"idine\", \"orine\", \"mycin\", \"exate\", \"amine\", \"emide\"]\n",
    "\n",
    "groups = [\"depressants\", \"steroid\", \"ceptives\", \"urates\", \"amines\", \"azines\", \"inhib\", \"coagul\", \"block\", \"acids\"]\n",
    "\n",
    "prove_group = \"NSAIDs\"\n",
    "prove_group_2 = \"SSRIs\"\n",
    "prove_group_3 = \"antacids\"\n",
    "\n",
    "prove_drug = \"alcohol\"\n",
    "prove_drug_2 = \"cimitidine\"\n",
    "prove_drug_3 = \"clozapine\"\n",
    "\n",
    "prove_brand = \"TAXOL\"\n",
    "prove_brand_2 = \"VIOXX\"\n",
    "\n",
    "#if (prove_brand_2.isupper() | prove_brand_2[0].isupper()) & (len(prove_brand) >= 4): \n",
    "#    print(\"brand\")\n",
    "#else:\n",
    "#    print(\"No\")\n",
    "\n",
    "#if prove_drug_3[-4:] in fours: \n",
    "#    print(\"drug\")\n",
    "#else:\n",
    "#    print(\"False\")\n",
    "\n",
    "#if (True in [t in prove_group_3 for t in groups]) | (prove_group_3.isupper() & len(prove_group_3) < 4): print(\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Phenothiazines', 0, 13) group\n",
      "('butyrophenones', 19, 32) group\n",
      "('epinephrine', 80, 90) drug\n"
     ]
    }
   ],
   "source": [
    "sent_1 = \"Activation of an effector immediate-early gene arc by methamphetamine\"\n",
    "for t in sent:\n",
    "    (is_brand_drug_group, type_text) = token_type_classifier(t[0])\n",
    "    #print(is_brand_drug_group, type_text)\n",
    "    \n",
    "    if is_brand_drug_group:\n",
    "        print(t, type_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8\n"
     ]
    }
   ],
   "source": [
    "# ------ Drug db tests -----------\n",
    "def old_token_type_classifier(word):\n",
    "    \n",
    "    threes = [\"nol\", \"lol\", \"hol\", \"lam\", \"pam\"]\n",
    "    fours = [\"arin\", \"oxin\", \"toin\",\"pine\", \"tine\", \"bital\", \"inol\", \"pram\"]\n",
    "    fives = [\"azole\", \"idine\", \"orine\", \"mycin\", \"exate\", \"amine\", \"emide\"]\n",
    "\n",
    "    groups = [\"depressants\", \"steroid\", \"ceptives\", \"urates\", \"amines\", \"azines\", \n",
    "              \"inhib\", \"coagul\", \"block\", \"acids\", \"agent\"]\n",
    "    \n",
    "    if word.isupper() & (len(word) >= 4): \n",
    "        return True, \"brand\"  # add word[0].isupper ?\n",
    "    elif (word[-3:] in threes) | (word[-4:] in fours) | (word[-5:] in fives):\n",
    "        return True, \"drug\"\n",
    "    elif (True in [t in word for t in groups]) | ((word[-1:] == \"s\") & (word[-2].isupper())) | (word.isupper() & (len(word) < 4)): \n",
    "        return True, \"group\"\n",
    "    else: \n",
    "        return False, \"\"\n",
    "    \n",
    "def with_db_type_classifier(word):\n",
    "    \n",
    "    threes = [\"nol\", \"lol\", \"hol\", \"lam\", \"pam\"]\n",
    "    fours = [\"arin\", \"oxin\", \"toin\",\"pine\", \"tine\", \"bital\", \"inol\", \"pram\"]\n",
    "    fives = [\"azole\", \"idine\", \"orine\", \"mycin\", \"exate\", \"amine\", \"emide\"]\n",
    "\n",
    "    groups = [\"depressants\", \"steroid\", \"ceptives\", \"urates\", \"amines\", \"azines\", \n",
    "              \"inhib\", \"coagul\", \"block\", \"acids\", \"agent\"]\n",
    "    \n",
    "    if word.isupper() & (len(word) >= 4): \n",
    "        return True, \"brand\"  # add word[0].isupper ?\n",
    "    elif (word[-3:] in threes) | (word[-4:] in fours) | (word[-5:] in fives):\n",
    "        return True, \"drug\"\n",
    "    elif (True in [t in word for t in groups]) | ((word[-1:] == \"s\") & (word[-2].isupper())) | (word.isupper() & (len(word) < 4)): \n",
    "        return True, \"group\"\n",
    "    # added \n",
    "    elif word in drug_set:\n",
    "        return True, \"drug\"\n",
    "    else: \n",
    "        return False, \"\"\n",
    "    \n",
    "sent = \"\"\"Activation of an effector immediate-early gene arc by methamphetamine.\n",
    "Interactions between treatments with coumaphos, bishydroxycoumarin (an anticoagulant),\n",
    "trichlorfon (an organophosphorous compound), and phenobarbital sodium (an inducer of microsomal enzymes)\n",
    "were investigated in sheep. Maximal exercise testing, a maneuver often applied to cardiac patients,\n",
    "does not significantly alter the serum digoxin level.\"\"\"\n",
    "no_db_checking_count = 0\n",
    "with_db_checking_count = 0\n",
    "sent_1 = tokenize(sent)\n",
    "# without checking db:\n",
    "for t in sent_1:\n",
    "    (is_brand_drug_group, type_text) = old_token_type_classifier(t[0])\n",
    "    (is_brand_drug_group_with_db, type_text) = with_db_type_classifier(t[0])\n",
    "    \n",
    "    if is_brand_drug_group:\n",
    "        no_db_checking_count += 1\n",
    "    if is_brand_drug_group_with_db:\n",
    "        with_db_checking_count += 1\n",
    "\n",
    "print(no_db_checking_count, with_db_checking_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
