{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extractor\n",
    "- Must be an independent program, separate from learner and classifier.\n",
    "- Must get as argument the directory with the XML files to encode. \n",
    "- Must print the feature vectors to `stdout`\n",
    "\n",
    "> DDI-DrugBank.d658.s0 When 0 3 O form=When formlower=when suf3=hen\n",
    "suf4=When isTitle BoS formNext=administered\n",
    "formlowerNext=administered suf3Next=red suf4Next=ered\n",
    "\n",
    "> DDI-DrugBank.d658.s0 administered 5 16 O form=administered\n",
    "formlower=administered suf3=red suf4=ered formPrev=When\n",
    "formlowerPrev=when suf3Prev=hen suf4Prev=When isTitlePrev\n",
    "formNext=concurrently formlowerNext=concurrently suf3Next=tly\n",
    "suf4Next=ntly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't remove stopwords and punctuations here since they may be used for feature extracting\n",
    "def tokenize(s):\n",
    "    '''\n",
    "    Given a sentence , calls nltk.tokenize to split it in tokens, and adds to each token its start / end offset \n",
    "    in the original sentence .\n",
    "    Input - s: string containing the text for one sentence\n",
    "    Output - Returns a list of tuples (word , offsetFrom , offsetTo )\n",
    "    '''\n",
    "\n",
    "    token_list = []\n",
    "    tokens = word_tokenize(s)\n",
    "    \n",
    "    for t in tokens:\n",
    "            offsetFrom = s.find(t)\n",
    "            offsetTo = offsetFrom + len(t) - 1\n",
    "            token_list.append((t, offsetFrom, offsetTo))\n",
    "            \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "Given a tokenized sentence, return a feature vector fo each token.\n",
    "Example :\n",
    "> `extract_features` ([(\" Ascorbic \" ,0 ,7) , (\" acid \" ,9 ,12) , (\" ,\" ,13 ,13) ,\n",
    "(\" aspirin \" ,15 ,21) , (\" ,\" ,22 ,22) , (\" and \" ,24 ,26) , (\" the \" ,28 ,30) ,\n",
    "(\" common \" ,32 ,37) , (\" cold \" ,39 ,42) , (\".\" ,43 ,43) ])\n",
    "[ [ \" form = Ascorbic \", \" suf4 = rbic \", \" next = acid \", \" prev = _BoS_ \", \"\n",
    "capitalized \" ],\n",
    "\n",
    "> Output -> [ \" form = acid \", \" suf4 = acid \", \" next =,\", \" prev = Ascorbic \" ],\n",
    "[ \" form =,\", \" suf4 =,\", \" next = aspirin \", \" prev = acid \", \" punct \" ],\n",
    "[ \" form = aspirin \", \" suf4 = irin \", \" next =,\", \" prev =,\" ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = 'Ascorbic acid, aspirin, and the common cold'\n",
    "sent_2 = 'Phenothiazines and 3-butyrophenones may reduce or reverse the depressor effect of epinephrine'\n",
    "\n",
    "tokenized_sent_1 = tokenize(sent_1)\n",
    "tokenized_sent_2 = tokenize(sent_2)\n",
    "\n",
    "def has_numbers(word):\n",
    "    return any(l.isdigit() for l in word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tag\n",
    "Given a token and a list of ground truth entities in a sentence, decide which is the B-I-O tag for the token.\n",
    "\n",
    "**B-I-O Approach** = Mark each token as **B**egin of a **sub**sequence, **I**nside a subsequence, or **O**utside any subsequence.\n",
    "> `get_tag` ((\" Ascorbic \" ,0 ,7) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> B-drug\n",
    "\n",
    "> `get_tag` ((\" acid \" ,9 ,12) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> I-drug\n",
    "\n",
    "> `get_tag` ((\" common \" ,32 ,37) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> 0\n",
    "\n",
    "> `get_tag` ((\" aspirin \" ,15 ,21) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) --> B-brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(token, gold):\n",
    "    '''\n",
    "    Input:\n",
    "        token: A token, i.e. one triple (word, offsetFrom, offsetTo)\n",
    "        gold: A list of ground truth entities, i.e. a list of triples (offsetFrom, offsetTo, type)\n",
    "        \n",
    "    Output:\n",
    "        The B-I-O ground truth tag for the given token (\"B-drug\", \"I-drug\", \"B-group\", \"I-group\", \"O\", ...)\n",
    "    '''\n",
    "    (form, start, end) = token\n",
    "    for (offsetFrom, offsetTo, Type) in gold:\n",
    "        if start == offsetFrom and end<=offsetTo:\n",
    "            return \"B-\"+Type # First letter of token equals 0 -> Beginning\n",
    "        elif start > offsetFrom and end <=offsetTo:\n",
    "            return \"I-\"+Type # Word not in the beginning\n",
    "    return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B- drug \n",
      "I- drug \n",
      "O\n",
      "B- brand \n"
     ]
    }
   ],
   "source": [
    "print(get_tag ((\" Ascorbic \" ,0 ,7) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))\n",
    "print(get_tag ((\" acid \" ,9 ,12) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))\n",
    "print(get_tag ((\" common \" ,32 ,37) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) )\n",
    "print(get_tag ((\" aspirin \" ,15 ,21) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tokenized_sentence):\n",
    "    '''\n",
    "    Input:\n",
    "        s: A tokenized sentence (list of triples (word, offsetFrom, offsetTo) )\n",
    "        \n",
    "    Output: \n",
    "        A list of feature vectors, one per token.\n",
    "        Features are binary and vectors are in sparse representeation (i.e. only active features are listed)\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for i in range(0, len(tokenized_sentence)):\n",
    "        t = tokenized_sentence[i][0]\n",
    "        punct = [\".\",\",\",\";\",\":\",\"?\",\"!\"]\n",
    "        \n",
    "        # length, number of digits, rules \n",
    "        \n",
    "        tokenFeatures = [\n",
    "            \"form=\" + t,\n",
    "            \"formlower=\" + t.lower(),\n",
    "            \"suf3=\" + t[-3:],\n",
    "            \"suf4=\" + t[-4:],\n",
    "            \"capitalized=%s \" % t.istitle(),\n",
    "            \"uppercase=%s\" % t.isupper(),\n",
    "            \"digit=%s\" % t.isdigit(),\n",
    "            \"hasNumber=%s\" % has_numbers(t),\n",
    "            \"stopword=%s\" % (t in stopwords),\n",
    "            \"punctuation=%s\" % (t in punct),\n",
    "            #\"posTag = %s\" % pos_tag(t, tagset = 'universal')[0][1]\n",
    "        ]  \n",
    "        \n",
    "\n",
    "            \n",
    "        features.append(tokenFeatures)\n",
    "        \n",
    "    for i, current_token in enumerate(features):\n",
    "        # add previous token\n",
    "        if i > 0:\n",
    "            prev_token = features[i-1]\n",
    "            current_token.append(\"prev=%s\" % prev_token[0][5:])\n",
    "        else:\n",
    "            current_token.append(\"prev=_BoS_\") #beginning of sentence?\n",
    "            \n",
    "        # add next token\n",
    "        if i < len(features)-1:\n",
    "            next_token = features[i+1]\n",
    "            current_token.append(\"next=%s\" % next_token[0][5:])\n",
    "        else:\n",
    "            current_token.append(\"next=_EoS_\") # end of sentence\n",
    "\n",
    "        # we could also add the suffixes of the previous/next word\n",
    "            \n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['form=Ascorbic', 'formlower=ascorbic', 'suf3=bic', 'suf4=rbic', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=_BoS_', 'next=acid'], ['form=acid', 'formlower=acid', 'suf3=cid', 'suf4=acid', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=Ascorbic', 'next=,'], ['form=,', 'formlower=,', 'suf3=,', 'suf4=,', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=acid', 'next=aspirin'], ['form=aspirin', 'formlower=aspirin', 'suf3=rin', 'suf4=irin', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=,', 'next=,'], ['form=,', 'formlower=,', 'suf3=,', 'suf4=,', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=aspirin', 'next=and'], ['form=and', 'formlower=and', 'suf3=and', 'suf4=and', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=,', 'next=the'], ['form=the', 'formlower=the', 'suf3=the', 'suf4=the', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=and', 'next=common'], ['form=common', 'formlower=common', 'suf3=mon', 'suf4=mmon', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=the', 'next=cold'], ['form=cold', 'formlower=cold', 'suf3=old', 'suf4=cold', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=common', 'next=_EoS_']]\n"
     ]
    }
   ],
   "source": [
    "feats_sent_1 = extract_features(tokenized_sent_1)\n",
    "feats_sent_2 = extract_features(tokenized_sent_2)\n",
    "print(feats_sent_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/Users/mponsclo/Downloads/labAHLT/data/train'\n",
    "datadir = \"../labAHLT/data/train\"\n",
    "\n",
    "\n",
    "\n",
    "def feature_extractor(datadir, resultpath):\n",
    "    result_f = open(resultpath, 'w')\n",
    "    # process each file in directory\n",
    "    for f in listdir(datadir):\n",
    "\n",
    "        # parse XML file, obtaining a DOM tree\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "\n",
    "        # process each sentence in the file\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        for s in sentences:\n",
    "            sid = s.attributes[\"id\"].value # get sentence id\n",
    "            stext = s.attributes[\"text\"].value # get sentence text\n",
    "            # load ground truth entities\n",
    "            gold = []\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                # for discontinuous entities, we only get the first span\n",
    "                offset = e.attributes[\"charOffset\"].value      # e.g. 24-44\n",
    "                try: # too many values to unpack in some iteration\n",
    "                    (start, end) = offset.split(\":\")[0].split(\"-\") # e.g. start:24, end:44\n",
    "                except:\n",
    "                    pass\n",
    "                gold.append((int(start), int(end), e.attributes[\"type\"].value)) # e.g. [(24, 44, 'drug')] \n",
    "\n",
    "            # tokenize text\n",
    "            tokens = tokenize(stext)\n",
    "\n",
    "            # extract features for each word in the sentence\n",
    "            features = extract_features(tokens)\n",
    "\n",
    "            # print features in format suitable for the learner/classifier\n",
    "            for i in range (0, len(tokens)):\n",
    "                # see if the token is part of an entity, and which part (B/I)\n",
    "                tag = get_tag(tokens[i], gold)\n",
    "                joined_features = \"\\t\".join(features[i])\n",
    "                result_f.write(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\n\".format(sid, tokens[i][0], tokens[i][1], tokens[i][2], tag, joined_features))\n",
    "                #print(sid, tokens[i][0], tokens[i][1], tokens[i][2], tag, \"\\t\".join(features[i]), sep='\\t')\n",
    "        \n",
    "\n",
    "            # black line to separate sentences\n",
    "            #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "feature_extractor() missing 1 required positional argument: 'resultpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-05aaa3061a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: feature_extractor() missing 1 required positional argument: 'resultpath'"
     ]
    }
   ],
   "source": [
    "feature_extractor(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: CRF\n",
    "The learner needs only the right class and the features, so you'll need to remove the 4 extra fields _(sent\\_id, token, span\\_start, span\\_end)_ added by the feature extractor, before feeding the vector to the learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindatadir = \"../labAHLT/data/train\"\n",
    "testdatadir = \"../labAHLT/data/test\"\n",
    "\n",
    "train_data_filename = \"train_data.out\"\n",
    "test_data_filename = \"test_data.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor(traindatadir, train_data_filename)\n",
    "feature_extractor(testdatadir, test_data_filename)\n",
    "#todo read about features accepted by CRF & try fitting crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "# def read_feature_file(filepath):\n",
    "#     f = open(filepath, 'r')\n",
    "#     lines = f.readlines()\n",
    "#     features = {}\n",
    "#     classes = {}\n",
    "#     metadata={} # sentence ids, token form, offsets - for later reconstruction\n",
    "#     #print(lines)\n",
    "#     for line in lines:\n",
    "#         split_line = line[:-1].split('\\t')\n",
    "#         sentence_id = split_line[0]\n",
    "#         feature_dict = {\n",
    "#             \"form\": split_line[5],\n",
    "#             \"formlower\": split_line[6]\n",
    "            \n",
    "#         }\n",
    "#         #TODO read all features from text files\n",
    "#         metadata.append((sentence_id, (split_line[1], split_line[2], split_line[3])))\n",
    "#         #features.append(split_line[5:])\n",
    "#         features.append(feature_dict)\n",
    "#         classes.append(split_line[4])\n",
    "#     return metadata, features, classes\n",
    "def sentence2features(sentence):\n",
    "    classes = []\n",
    "    features = []\n",
    "    tokens = []\n",
    "    for token in sentence:\n",
    "        split_line = token[:-1].split('\\t')\n",
    "        sentence_id = split_line[0]\n",
    "        feature_dict = {\n",
    "            \"form\": split_line[5],\n",
    "            \"formlower\": split_line[6]\n",
    "\n",
    "        }\n",
    "        #TODO read all features from text files\n",
    "        features.append(split_line[5:])\n",
    "        classes.append(split_line[4])\n",
    "        tokens.append((split_line[1], split_line[2], split_line[3]))\n",
    "    return features, classes, tokens\n",
    "\n",
    "def read_feature_file(filepath):\n",
    "    f = open(filepath, 'r')\n",
    "    lines = f.readlines()\n",
    "    features = []\n",
    "    classes = []\n",
    "    metadata=[]# sentence ids, token form, offsets - for later reconstruction\n",
    "    sentences = set()\n",
    "    #print(lines)\n",
    "    sentences = groupby(lines, lambda l: l.split('\\t')[0])\n",
    "    for sid, sentence in sentences:\n",
    "        s_features, s_classes, s_tokens = sentence2features(sentence)\n",
    "        features.append(s_features)\n",
    "        classes.append(s_classes)\n",
    "        metadata.append((sid, s_tokens))\n",
    "           \n",
    "    return metadata, features, classes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata, X_train, y_train = read_feature_file(train_data_filename)\n",
    "test_metadata, X_test, y_test = read_feature_file(test_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['form=While', 'formlower=while', 'suf3=ile', 'suf4=hile', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=_BoS_', 'next=co-administration'], ['form=co-administration', 'formlower=co-administration', 'suf3=ion', 'suf4=tion', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=While', 'next=of'], ['form=of', 'formlower=of', 'suf3=of', 'suf4=of', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=co-administration', 'next=ZAVESCA'], ['form=ZAVESCA', 'formlower=zavesca', 'suf3=SCA', 'suf4=ESCA', 'capitalized=False ', 'uppercase=True', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=of', 'next=appeared'], ['form=appeared', 'formlower=appeared', 'suf3=red', 'suf4=ared', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=ZAVESCA', 'next=to'], ['form=to', 'formlower=to', 'suf3=to', 'suf4=to', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=appeared', 'next=increase'], ['form=increase', 'formlower=increase', 'suf3=ase', 'suf4=ease', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=to', 'next=the'], ['form=the', 'formlower=the', 'suf3=the', 'suf4=the', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=increase', 'next=clearance'], ['form=clearance', 'formlower=clearance', 'suf3=nce', 'suf4=ance', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=the', 'next=of'], ['form=of', 'formlower=of', 'suf3=of', 'suf4=of', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=clearance', 'next=Cerezyme'], ['form=Cerezyme', 'formlower=cerezyme', 'suf3=yme', 'suf4=zyme', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=of', 'next=by'], ['form=by', 'formlower=by', 'suf3=by', 'suf4=by', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=Cerezyme', 'next=70'], ['form=70', 'formlower=70', 'suf3=70', 'suf4=70', 'capitalized=False ', 'uppercase=False', 'digit=True', 'hasNumber=True', 'stopword=False', 'punctuation=False', 'prev=by', 'next=%'], ['form=%', 'formlower=%', 'suf3=%', 'suf4=%', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=70', 'next=,'], ['form=,', 'formlower=,', 'suf3=,', 'suf4=,', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=%', 'next=these'], ['form=these', 'formlower=these', 'suf3=ese', 'suf4=hese', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=,', 'next=results'], ['form=results', 'formlower=results', 'suf3=lts', 'suf4=ults', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=these', 'next=are'], ['form=are', 'formlower=are', 'suf3=are', 'suf4=are', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=results', 'next=not'], ['form=not', 'formlower=not', 'suf3=not', 'suf4=not', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=are', 'next=conclusive'], ['form=conclusive', 'formlower=conclusive', 'suf3=ive', 'suf4=sive', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=not', 'next=because'], ['form=because', 'formlower=because', 'suf3=use', 'suf4=ause', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=conclusive', 'next=of'], ['form=of', 'formlower=of', 'suf3=of', 'suf4=of', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=because', 'next=the'], ['form=the', 'formlower=the', 'suf3=the', 'suf4=the', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=of', 'next=small'], ['form=small', 'formlower=small', 'suf3=all', 'suf4=mall', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=the', 'next=number'], ['form=number', 'formlower=number', 'suf3=ber', 'suf4=mber', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=small', 'next=of'], ['form=of', 'formlower=of', 'suf3=of', 'suf4=of', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=number', 'next=subjects'], ['form=subjects', 'formlower=subjects', 'suf3=cts', 'suf4=ects', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=of', 'next=studied'], ['form=studied', 'formlower=studied', 'suf3=ied', 'suf4=died', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=subjects', 'next=and'], ['form=and', 'formlower=and', 'suf3=and', 'suf4=and', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=studied', 'next=because'], ['form=because', 'formlower=because', 'suf3=use', 'suf4=ause', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=and', 'next=patients'], ['form=patients', 'formlower=patients', 'suf3=nts', 'suf4=ents', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=because', 'next=took'], ['form=took', 'formlower=took', 'suf3=ook', 'suf4=took', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=patients', 'next=variable'], ['form=variable', 'formlower=variable', 'suf3=ble', 'suf4=able', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=took', 'next=doses'], ['form=doses', 'formlower=doses', 'suf3=ses', 'suf4=oses', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=variable', 'next=of'], ['form=of', 'formlower=of', 'suf3=of', 'suf4=of', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=doses', 'next=Cerezyme'], ['form=Cerezyme', 'formlower=cerezyme', 'suf3=yme', 'suf4=zyme', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=of', 'next=.'], ['form=.', 'formlower=.', 'suf3=.', 'suf4=.', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=Cerezyme', 'next=_EoS_']], [['form=Combination', 'formlower=combination', 'suf3=ion', 'suf4=tion', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=_BoS_', 'next=therapy'], ['form=therapy', 'formlower=therapy', 'suf3=apy', 'suf4=rapy', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=Combination', 'next=with'], ['form=with', 'formlower=with', 'suf3=ith', 'suf4=with', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=therapy', 'next=Cerezyme'], ['form=Cerezyme', 'formlower=cerezyme', 'suf3=yme', 'suf4=zyme', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=with', 'next=('], ['form=(', 'formlower=(', 'suf3=(', 'suf4=(', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=Cerezyme', 'next=imiglucerase'], ['form=imiglucerase', 'formlower=imiglucerase', 'suf3=ase', 'suf4=rase', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=(', 'next=)'], ['form=)', 'formlower=)', 'suf3=)', 'suf4=)', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=imiglucerase', 'next=and'], ['form=and', 'formlower=and', 'suf3=and', 'suf4=and', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=)', 'next=ZAVESCA'], ['form=ZAVESCA', 'formlower=zavesca', 'suf3=SCA', 'suf4=ESCA', 'capitalized=False ', 'uppercase=True', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=and', 'next=is'], ['form=is', 'formlower=is', 'suf3=is', 'suf4=is', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=ZAVESCA', 'next=not'], ['form=not', 'formlower=not', 'suf3=not', 'suf4=not', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=is', 'next=indicated'], ['form=indicated', 'formlower=indicated', 'suf3=ted', 'suf4=ated', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=not', 'next=.'], ['form=.', 'formlower=.', 'suf3=.', 'suf4=.', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=indicated', 'next=_EoS_']], [['form=No', 'formlower=no', 'suf3=No', 'suf4=No', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=_BoS_', 'next=drug'], ['form=drug', 'formlower=drug', 'suf3=rug', 'suf4=drug', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=No', 'next=interactions'], ['form=interactions', 'formlower=interactions', 'suf3=ons', 'suf4=ions', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=drug', 'next=have'], ['form=have', 'formlower=have', 'suf3=ave', 'suf4=have', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=interactions', 'next=been'], ['form=been', 'formlower=been', 'suf3=een', 'suf4=been', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=have', 'next=reported'], ['form=reported', 'formlower=reported', 'suf3=ted', 'suf4=rted', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=been', 'next=between'], ['form=between', 'formlower=between', 'suf3=een', 'suf4=ween', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=reported', 'next=Prostin'], ['form=Prostin', 'formlower=prostin', 'suf3=tin', 'suf4=stin', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=between', 'next=VR'], ['form=VR', 'formlower=vr', 'suf3=VR', 'suf4=VR', 'capitalized=False ', 'uppercase=True', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=Prostin', 'next=Pediatric'], ['form=Pediatric', 'formlower=pediatric', 'suf3=ric', 'suf4=tric', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=VR', 'next=and'], ['form=and', 'formlower=and', 'suf3=and', 'suf4=and', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=Pediatric', 'next=the'], ['form=the', 'formlower=the', 'suf3=the', 'suf4=the', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=and', 'next=therapy'], ['form=therapy', 'formlower=therapy', 'suf3=apy', 'suf4=rapy', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=the', 'next=standard'], ['form=standard', 'formlower=standard', 'suf3=ard', 'suf4=dard', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=therapy', 'next=in'], ['form=in', 'formlower=in', 'suf3=in', 'suf4=in', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=standard', 'next=neonates'], ['form=neonates', 'formlower=neonates', 'suf3=tes', 'suf4=ates', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=in', 'next=with'], ['form=with', 'formlower=with', 'suf3=ith', 'suf4=with', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=neonates', 'next=restricted'], ['form=restricted', 'formlower=restricted', 'suf3=ted', 'suf4=cted', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=with', 'next=pulmonary'], ['form=pulmonary', 'formlower=pulmonary', 'suf3=ary', 'suf4=nary', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=restricted', 'next=or'], ['form=or', 'formlower=or', 'suf3=or', 'suf4=or', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=pulmonary', 'next=systemic'], ['form=systemic', 'formlower=systemic', 'suf3=mic', 'suf4=emic', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=or', 'next=blood'], ['form=blood', 'formlower=blood', 'suf3=ood', 'suf4=lood', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=systemic', 'next=flow'], ['form=flow', 'formlower=flow', 'suf3=low', 'suf4=flow', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=blood', 'next=.'], ['form=.', 'formlower=.', 'suf3=.', 'suf4=.', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=flow', 'next=_EoS_']], [['form=Standard', 'formlower=standard', 'suf3=ard', 'suf4=dard', 'capitalized=True ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=_BoS_', 'next=therapy'], ['form=therapy', 'formlower=therapy', 'suf3=apy', 'suf4=rapy', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=Standard', 'next=includes'], ['form=includes', 'formlower=includes', 'suf3=des', 'suf4=udes', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=therapy', 'next=antibiotics'], ['form=antibiotics', 'formlower=antibiotics', 'suf3=ics', 'suf4=tics', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=includes', 'next=,'], ['form=,', 'formlower=,', 'suf3=,', 'suf4=,', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=antibiotics', 'next=such'], ['form=such', 'formlower=such', 'suf3=uch', 'suf4=such', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=,', 'next=as'], ['form=as', 'formlower=as', 'suf3=as', 'suf4=as', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=such', 'next=penicillin'], ['form=penicillin', 'formlower=penicillin', 'suf3=lin', 'suf4=llin', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=as', 'next=and'], ['form=and', 'formlower=and', 'suf3=and', 'suf4=and', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=penicillin', 'next=gentamicin'], ['form=gentamicin', 'formlower=gentamicin', 'suf3=cin', 'suf4=icin', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=and', 'next=;'], ['form=;', 'formlower=;', 'suf3=;', 'suf4=;', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=gentamicin', 'next=_EoS_']], [['form=vasopressors', 'formlower=vasopressors', 'suf3=ors', 'suf4=sors', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=_BoS_', 'next=,'], ['form=,', 'formlower=,', 'suf3=,', 'suf4=,', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=vasopressors', 'next=such'], ['form=such', 'formlower=such', 'suf3=uch', 'suf4=such', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=,', 'next=as'], ['form=as', 'formlower=as', 'suf3=as', 'suf4=as', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=such', 'next=dopamine'], ['form=dopamine', 'formlower=dopamine', 'suf3=ine', 'suf4=mine', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=as', 'next=and'], ['form=and', 'formlower=and', 'suf3=and', 'suf4=and', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=True', 'punctuation=False', 'prev=dopamine', 'next=isoproterenol'], ['form=isoproterenol', 'formlower=isoproterenol', 'suf3=nol', 'suf4=enol', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=False', 'prev=and', 'next=;'], ['form=;', 'formlower=;', 'suf3=;', 'suf4=;', 'capitalized=False ', 'uppercase=False', 'digit=False', 'hasNumber=False', 'stopword=False', 'punctuation=True', 'prev=isoproterenol', 'next=_EoS_']]]\n",
      "[['O', 'O', 'O', 'B-brand', 'O', 'O', 'O', 'O', 'O', 'O', 'B-brand', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-brand', 'O'], ['O', 'O', 'O', 'B-brand', 'O', 'B-drug', 'O', 'O', 'B-brand', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-brand', 'I-brand', 'I-brand', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-group', 'O', 'O', 'O', 'B-drug', 'O', 'B-drug', 'O'], ['B-group', 'O', 'O', 'I-group', 'B-drug', 'O', 'B-drug', 'O']]\n",
      "[('DDI-DrugBank.d695.s0', [('While', '0', '4'), ('co-administration', '6', '22'), ('of', '24', '25'), ('ZAVESCA', '27', '33'), ('appeared', '38', '45'), ('to', '47', '48'), ('increase', '50', '57'), ('the', '59', '61'), ('clearance', '63', '71'), ('of', '24', '25'), ('Cerezyme', '76', '83'), ('by', '85', '86'), ('70', '88', '89'), ('%', '90', '90'), (',', '91', '91'), ('these', '93', '97'), ('results', '99', '105'), ('are', '42', '44'), ('not', '111', '113'), ('conclusive', '115', '124'), ('because', '126', '132'), ('of', '24', '25'), ('the', '59', '61'), ('small', '141', '145'), ('number', '147', '152'), ('of', '24', '25'), ('subjects', '157', '164'), ('studied', '166', '172'), ('and', '174', '176'), ('because', '126', '132'), ('patients', '186', '193'), ('took', '195', '198'), ('variable', '200', '207'), ('doses', '209', '213'), ('of', '24', '25'), ('Cerezyme', '76', '83'), ('.', '226', '226')]), ('DDI-DrugBank.d695.s1', [('Combination', '0', '10'), ('therapy', '12', '18'), ('with', '20', '23'), ('Cerezyme', '25', '32'), ('(', '37', '37'), ('imiglucerase', '38', '49'), (')', '50', '50'), ('and', '52', '54'), ('ZAVESCA', '56', '62'), ('is', '67', '68'), ('not', '70', '72'), ('indicated', '74', '82'), ('.', '83', '83')]), ('DDI-DrugBank.d290.s0', [('No', '0', '1'), ('drug', '3', '6'), ('interactions', '8', '19'), ('have', '21', '24'), ('been', '26', '29'), ('reported', '31', '38'), ('between', '40', '46'), ('Prostin', '48', '54'), ('VR', '56', '57'), ('Pediatric', '59', '67'), ('and', '69', '71'), ('the', '73', '75'), ('therapy', '77', '83'), ('standard', '85', '92'), ('in', '8', '9'), ('neonates', '97', '104'), ('with', '106', '109'), ('restricted', '111', '120'), ('pulmonary', '122', '130'), ('or', '34', '35'), ('systemic', '135', '142'), ('blood', '144', '148'), ('flow', '150', '153'), ('.', '154', '154')]), ('DDI-DrugBank.d290.s1', [('Standard', '0', '7'), ('therapy', '9', '15'), ('includes', '17', '24'), ('antibiotics', '26', '36'), (',', '37', '37'), ('such', '39', '42'), ('as', '44', '45'), ('penicillin', '47', '56'), ('and', '2', '4'), ('gentamicin', '62', '71'), (';', '72', '72')]), ('DDI-DrugBank.d290.s2', [('vasopressors', '0', '11'), (',', '12', '12'), ('such', '14', '17'), ('as', '1', '2'), ('dopamine', '22', '29'), ('and', '31', '33'), ('isoproterenol', '35', '47'), (';', '48', '48')])]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])\n",
    "print(y_train[:5])\n",
    "print(train_metadata[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_result_file = 'conll2002-esp.crfsuite'\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "trainer.train(train_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 50,\n",
       " 'scores': {},\n",
       " 'loss': 15116.108667,\n",
       " 'feature_norm': 85.609124,\n",
       " 'error_norm': 277.056944,\n",
       " 'active_features': 6909,\n",
       " 'linesearch_trials': 1,\n",
       " 'linesearch_step': 1.0,\n",
       " 'time': 0.069}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 {'num': 50, 'scores': {}, 'loss': 15116.108667, 'feature_norm': 85.609124, 'error_norm': 277.056944, 'active_features': 6909, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.069}\n"
     ]
    }
   ],
   "source": [
    "print(len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: B-drug O O O O O O O O O O O O O O O B-drug O\n",
      "Correct:   B-drug O O O O O O I-drug O O O O O O O O B-drug_n O\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(train_result_file)\n",
    "\n",
    "predicted = tagger.tag(X_test[0])\n",
    "\n",
    "print(\"Predicted:\", ' '.join(predicted))\n",
    "print(\"Correct:  \", ' '.join(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
