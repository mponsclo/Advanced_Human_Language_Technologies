{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "\n",
    "    token_list = []\n",
    "    tokens = word_tokenize(s)\n",
    "    \n",
    "    for t in tokens:\n",
    "            offsetFrom = s.find(t)\n",
    "            offsetTo = offsetFrom + len(t) - 1\n",
    "            token_list.append((t, offsetFrom, offsetTo))\n",
    "            \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = 'Ascorbic acid, aspirin, and the common cold PCP'\n",
    "sent_2 = 'Phenothiazines and butyrophenones may reduce or reverse the depressor effect of epinephrine'\n",
    "\n",
    "def has_numbers(word):\n",
    "    return any(l.isdigit() for l in word)\n",
    "def num_digits(word):\n",
    "    return sum(l.isdigit() for l in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_DB_PATH = './resources/HSDB.txt'\n",
    "DRUG_BANK_PATH = './resources/DrugBank.txt'\n",
    "\n",
    "SimpleDrugDb = set()\n",
    "DrugBank = {'drug' : set(), 'brand': set(), 'group': set()}\n",
    "\n",
    "def read_drug_list_files():\n",
    "    '''\n",
    "    Read the drug databases.\n",
    "    '''\n",
    "    resource_file = open(SIMPLE_DB_PATH, 'r')\n",
    "    lines = resource_file.readlines()\n",
    "    global SimpleDrugDb\n",
    "    SimpleDrugDb = set([d[:-1].lower() for d in lines])\n",
    "\n",
    "    resource_file = open(DRUG_BANK_PATH, 'r')\n",
    "    lines = resource_file.readlines()\n",
    "    global DrugBank\n",
    "    split_lines = [(line.split('|')[0].lower(), line.split('|')[1][:-1]) for line in lines]\n",
    "\n",
    "    for name, n_type in split_lines:\n",
    "        DrugBank[n_type].add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_db_resources(word):\n",
    "    \n",
    "    drug_n = [\"PCP\", \"18-MC\", \"ibogaine\", \"MHD\", \"endotoxin\", \"toxin\", \"NANM\", \"ginsenosides\", \n",
    "             \"NaCMC\", \"PTX\", \"coumaphos\", \"contortrostatin\", \"resveratrol\", \"GSLS\", \"methylglyoxal\",\n",
    "             \"hydrodolasetron\", \"neurotensin\"]\n",
    "\n",
    "    if (word.lower() in SimpleDrugDb):\n",
    "        return True, \"drug\"\n",
    "    elif (word.lower() in DrugBank[\"drug\"]):\n",
    "        return True, \"drug\"\n",
    "    elif (word.lower() in DrugBank[\"brand\"]):\n",
    "        return True, \"brand\"\n",
    "    elif (word.lower() in DrugBank[\"group\"]):\n",
    "        return True, \"group\"\n",
    "    #elif (True in [word in word for word in drug_n]):\n",
    "        #return True, \"drug_n\"\n",
    "    else:\n",
    "        return False, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(token, gold):\n",
    "    '''\n",
    "    Input:\n",
    "        token: A token, i.e. one triple (word, offsetFrom, offsetTo)\n",
    "        gold: A list of ground truth entities, i.e. a list of triples (offsetFrom, offsetTo, type)\n",
    "        \n",
    "    Output:\n",
    "        The B-I-O ground truth tag for the given token (\"B-drug\", \"I-drug\", \"B-group\", \"I-group\", \"O\", ...)\n",
    "    '''\n",
    "    (form, start, end) = token\n",
    "    for (offsetFrom, offsetTo, Type) in gold:\n",
    "        if start == offsetFrom and end<=offsetTo:\n",
    "            return \"B-\"+Type # First letter of token equals 0 -> Beginning\n",
    "        elif start > offsetFrom and end <=offsetTo:\n",
    "            return \"I-\"+Type # Word not in the beginning\n",
    "    return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B- drug \n",
      "I- drug \n",
      "O\n",
      "B- brand \n"
     ]
    }
   ],
   "source": [
    "print(get_tag ((\" Ascorbic \" ,0 ,7) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))\n",
    "print(get_tag ((\" acid \" ,9 ,12) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))\n",
    "print(get_tag ((\" common \" ,32 ,37) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]) )\n",
    "print(get_tag ((\" aspirin \" ,15 ,21) , [(0 , 12, \" drug \") , (15 , 21, \" brand \") ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tokenized_sentence, should_look_up=False):\n",
    "    '''\n",
    "    Input:\n",
    "        s: A tokenized sentence (list of triples (word, offsetFrom, offsetTo) )\n",
    "        \n",
    "    Output: \n",
    "        A list of feature vectors, one per token.\n",
    "        Features are binary and vectors are in sparse representeation (i.e. only active features are listed)\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for i in range(0, len(tokenized_sentence)):\n",
    "        t = tokenized_sentence[i][0]\n",
    "        punct = [\".\",\",\",\";\",\":\",\"?\",\"!\"]\n",
    "        \n",
    "        # length, number of digits, rules \n",
    "        \n",
    "        tokenFeatures =  [\n",
    "            \"form=\" + t,\n",
    "            \"formlower=\" + t.lower(),\n",
    "            \"suf3=\" + t[-3:],\n",
    "            \"suf4=\" + t[-4:],\n",
    "            \"suf5=\" + t[-5:],\n",
    "            \"prfx3=\" + t[:3],\n",
    "            \"prfx4=\" + t[:4],\n",
    "            \"prfx5=\" + t[:5], \n",
    "            \"capitalized=%s \" % t.istitle(),\n",
    "            \"uppercase=%s\" % t.isupper(),\n",
    "            \"digit=%s\" % t.isdigit(),\n",
    "            \"stopword=%s\" % (t in stopwords),\n",
    "            \"punctuation=%s\" % (t in punct),\n",
    "            \"length=%s\" % len(t),\n",
    "            \"posTag=%s\" % pos_tag(t)[0][1], # ?\n",
    "            \"lemma=%s\" % wordnet_lemmatizer.lemmatize(t),\n",
    "            \"numDigits=%s\" % num_digits(t), \n",
    "            \"containsDash=%s\" % ('-' in t)\n",
    "        ]\n",
    "  \n",
    "        features.append(tokenFeatures)\n",
    "    \n",
    "        if should_look_up:\n",
    "            read_drug_list_files()\n",
    "            (is_drug, isType) = use_db_resources(t)\n",
    "            if is_drug: \n",
    "                tokenFeatures.append(\"Ruled = %s\" %isType) \n",
    "            else: \n",
    "                tokenFeatures.append(\"Ruled = O\") \n",
    "        \n",
    "    for i, current_token in enumerate(features):\n",
    "        # add previous token\n",
    "        if i > 0:\n",
    "            prev_token = features[i-1][0][5:]\n",
    "            current_token.append(\"prev=%s\" % prev_token)\n",
    "            current_token.append(\"suf3Prev = %s\" % prev_token[-3:])\n",
    "            current_token.append(\"suf4Prev = %s\" % prev_token[-4:])\n",
    "            current_token.append(\"prevIsTitle = %s\" % prev_token.istitle())\n",
    "            current_token.append(\"prevIsUpper = %s\" % prev_token.isupper())\n",
    "            current_token.append(\"PrevIsDigit = %s\" % prev_token.isdigit())\n",
    "\n",
    "        else:\n",
    "            current_token.append(\"prev=_BoS_\") #beginning of sentence\n",
    "            \n",
    "        # add next token\n",
    "        if i < len(features)-1:\n",
    "            next_token = features[i+1][0][5:]\n",
    "            current_token.append(\"next=%s\" % next_token)\n",
    "            current_token.append(\"suf3Next = %s\" % next_token[-3:])\n",
    "            current_token.append(\"suf4Next = %s\" % next_token[-3:])\n",
    "            current_token.append(\"NextIsTitle = %s\" % next_token.istitle())\n",
    "            current_token.append(\"NextIsUpper = %s\" % next_token.isupper())\n",
    "            current_token.append(\"NextIsDigit = %s\" % next_token.isdigit())\n",
    "\n",
    "        else:\n",
    "            current_token.append(\"next=_EoS_\") # end of sentence\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent_2 = tokenize(sent_2)\n",
    "feats_sent_2 = extract_features(tokenized_sent_2, should_look_up = True)\n",
    "#print(feats_sent_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(datadir, resultpath, should_look_up = False):\n",
    "    result_f = open(resultpath, 'w')\n",
    "    # process each file in directory\n",
    "    for f in listdir(datadir):\n",
    "\n",
    "        # parse XML file, obtaining a DOM tree\n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "\n",
    "        # process each sentence in the file\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "        for s in sentences:\n",
    "            sid = s.attributes[\"id\"].value # get sentence id\n",
    "            stext = s.attributes[\"text\"].value # get sentence text\n",
    "            # load ground truth entities\n",
    "            gold = []\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                # for discontinuous entities, we only get the first span\n",
    "                offset = e.attributes[\"charOffset\"].value      # e.g. 24-44\n",
    "                try: # too many values to unpack in some iteration\n",
    "                    (start, end) = offset.split(\":\")[0].split(\"-\") # e.g. start:24, end:44\n",
    "                except:\n",
    "                    pass\n",
    "                gold.append((int(start), int(end), e.attributes[\"type\"].value)) # e.g. [(24, 44, 'drug')] \n",
    "\n",
    "            # tokenize text\n",
    "            tokens = tokenize(stext)\n",
    "\n",
    "            # extract features for each word in the sentence\n",
    "            features = extract_features(tokens, should_look_up)\n",
    "\n",
    "            # print features in format suitable for the learner/classifier\n",
    "            for i in range (0, len(tokens)):\n",
    "                # see if the token is part of an entity, and which part (B/I)\n",
    "                tag = get_tag(tokens[i], gold)\n",
    "                joined_features = \"\\t\".join(features[i])\n",
    "                result_f.write(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\n\".format(sid, tokens[i][0], tokens[i][1], tokens[i][2], tag, joined_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = '/Users/mponsclo/Downloads/labAHLT/data/train'\n",
    "devel_dataset = '/Users/mponsclo/Downloads/labAHLT/data/devel'\n",
    "test_dataset = '/Users/mponsclo/Downloads/labAHLT/data/test'\n",
    "\n",
    "feature_extractor(train_dataset, 'features_train_l.txt', should_look_up = True)\n",
    "#feature_extractor(devel_dataset, 'features_devel')\n",
    "#feature_extractor(test_dataset, 'features_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner: CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import scipy\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_strings(sentence):\n",
    "    '''\n",
    "    Task:\n",
    "        Given a stringified sentence, parse it and return the data in relevant data structures.\n",
    "    Input:\n",
    "        sentence: list of strings representing the given sentence, where each piece of information is separated by a tab, e.g.\n",
    "            \"DDI-DrugBank.d695.s0\tWhile\t0\t4\tO\tform=While\tformlower=while\tsuf3=ile\tsuf4=hile\"\n",
    "    Output:\n",
    "        tokens: list of tuples representing tokens: (word, offset_from, offset_to)\n",
    "        features: list of features for every token\n",
    "        tags: list of tags (e.g. B-drug, O, I-brand etc.) for every token\n",
    "    '''\n",
    "    tags = []\n",
    "    features = []\n",
    "    tokens = []\n",
    "    for token in sentence:\n",
    "        split_data = token[:-1].split('\\t')\n",
    "        sentence_id = split_data[0]\n",
    "        features.append(split_data[5:])\n",
    "        tags.append(split_data[4])\n",
    "        tokens.append((split_data[1], split_data[2], split_data[3]))\n",
    "    return tokens, features, tags\n",
    "\n",
    "def read_feature_file(filepath):\n",
    "    '''\n",
    "    Task:\n",
    "        Given the path to the file containing tokenized sentences, read it and return the necessary data structures\n",
    "    Input:\n",
    "        filepath: Path to the data\n",
    "    Output:\n",
    "        tokens_by_sentence: list of tuples: (sentence_id, list_of_tokens). Each tuple represents a sentence,\n",
    "            where in the list_of_tokens each token is represented by a tuple (word, offset_from, offset_to)\n",
    "        features: list of lists of features per sentence\n",
    "        tags: list of lists of B-I-O tags per sentence\n",
    "    '''\n",
    "    features = []\n",
    "    tags = []\n",
    "    tokens_by_sentence=[]\n",
    "    \n",
    "    f = open(filepath, 'r')\n",
    "    lines = f.readlines()\n",
    "\n",
    "    # group the tokens by sentence\n",
    "    sentences = groupby(lines, lambda l: l.split('\\t')[0])\n",
    "\n",
    "    # process each sentence\n",
    "    for sid, sentence in sentences:\n",
    "        s_tokens, s_features, s_tags = parse_sentence_strings(sentence)\n",
    "        \n",
    "        tokens_by_sentence.append((sid, s_tokens))\n",
    "        features.append(s_features)\n",
    "        tags.append(s_tags)\n",
    "           \n",
    "    return tokens_by_sentence, features, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'type',\n",
       " 'c',\n",
       " 'error_sensitive',\n",
       " 'averaging',\n",
       " 'max_iterations',\n",
       " 'epsilon']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pycrfsuite.Trainer(algorithm='pa', verbose=False)\n",
    "#help(pycrfsuite.Trainer) \n",
    "pycrfsuite.Trainer.params(trainer)\n",
    "# lbfgs - Gradient descent using the L-BFGS method\n",
    "# l2sgd - Stochastic Gradient Descent with L2 regularization term\n",
    "# pa - Passive Aggressive\n",
    "params = {\n",
    "        'c': 0.21600273890535607,\n",
    "        'epsilon': 0.004802939229551229,\n",
    "        'type': 2,\n",
    "        'feature.possible_transitions': True,\n",
    "        'feature.possible_states': True,\n",
    "        'max_iterations': 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_learn(features_file, model_name):\n",
    "    \n",
    "    # Read features from file\n",
    "    _, X_train, y_train = read_feature_file(features_file)\n",
    "    \n",
    "    trainer = pycrfsuite.Trainer(algorithm='pa', verbose=False)\n",
    "    for xseq, yseq in zip(X_train, y_train):\n",
    "        trainer.append(xseq, yseq)\n",
    "        \n",
    "    trainer.set_params({\n",
    "        'c1': 0.2, \n",
    "        'c2': 0.001,\n",
    "        'max_iterations': 1000,\n",
    "        'feature.possible_states': True,\n",
    "        \n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    \n",
    "    trainer.train(model_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_learn(\"features_train\", \"ml_model.crfsuite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(sid, tokens, tags, outf):\n",
    "    '''\n",
    "    Task:\n",
    "        Given a list of tokens and the B-I-O tag for each token, produce a list\n",
    "            of drugs in the format expected by the evaluator.\n",
    "    Input:\n",
    "        sid: sentence identifier (required by the evaluator output format)\n",
    "        tokens: List of tokens in the sentence , i.e. list of tuples (word,\n",
    "            offsetFrom, offsetTo)\n",
    "        tags: List of B-I-O tags for each token\n",
    "    Output:\n",
    "        Prints to stdout the entities in the right format: one line per entity,\n",
    "        fields separated by ’|’, field order : id, offset, name, type.\n",
    "    '''\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        entity, offset_from, offset_to = tokens[i]\n",
    "        tag = tags[i]\n",
    "        \n",
    "        if tag[0] == 'B':\n",
    "            tag_name = tag[2:]\n",
    "            # in case the entity is longer than one word, concatenate it\n",
    "            j = i + 1\n",
    "            while j < len(tokens):\n",
    "                word_next, offset_from_next, offset_to_next = tokens[j]\n",
    "                tag_next = tags[j]\n",
    "                j += 1\n",
    "                if int(offset_from_next) - int(offset_to) != 2 or tag_next[0] != 'I':\n",
    "                    break\n",
    "                if tag_next[2:] == tag_name:\n",
    "                    entity = entity + ' ' + word_next\n",
    "                    offset_to = offset_to_next\n",
    "            # print the whole entity\n",
    "            outf.write(str(sid) + \"|\" + str(offset_from) + '-' + str(offset_to) + \"|\" + entity + \"|\" + tag_name)\n",
    "            \n",
    "            outf.write(\"\\n\")\n",
    "            \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_classifier(model_name, features, inputdir, outputfile):\n",
    "    \n",
    "    outf = open(outputfile, \"w\")\n",
    "        \n",
    "    ids_and_tokens, X_devel, y_devel = read_feature_file(features)\n",
    "    \n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(model_name)\n",
    "    tags = [tagger.tag(sentence) for sentence in X_devel]\n",
    "    \n",
    "    for sentence_tags, sentence_data in zip(tags, ids_and_tokens):\n",
    "        sid, tokens = sentence_data\n",
    "        output_entities(sid, tokens, sentence_tags, outf)\n",
    "        \n",
    "    outf.close()\n",
    "    evaluate(\"NER\", inputdir, outputfile)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ml_model.crfsuite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-e5ecf7c8279e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ml_model.crfsuite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"features_test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-925a9e3aa405>\u001b[0m in \u001b[0;36mcrf_classifier\u001b[0;34m(model_name, features, inputdir, outputfile)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpycrfsuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_devel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger._check_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ml_model.crfsuite'"
     ]
    }
   ],
   "source": [
    "result = crf_classifier(\"ml_model.crfsuite\", \"features_test.txt\", test_dataset, \"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MiriQ3",
   "language": "python",
   "name": "miriq3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
