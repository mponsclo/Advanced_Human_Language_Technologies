{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "import pickle\n",
    "from xml.dom.minidom import parse\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import *\n",
    "#from keras_contrib.layers import CRF\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "from evaluator import *\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed(2020)\n",
    "set_seed(2021)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_data()`- Use XML parsing and tokenization functions from previous exercises. Adding a PoS tagger or lemmatizer may be useful. \\\n",
    "Masking the target drugs as e.g. `<DRUG1>`, `<DRUG2>`, and the rest as `<DRUG_OTHER>` will help the algorithm generalize and avoid it focusing in the drug names, which are not relevant for the DDI task (and also make it easier for it to spot the target entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    '''\n",
    "    Task: Load XML files in given directory, tokenize each sentence, and extract\n",
    "    learning examples (tokenized sentence + entity pair).\n",
    "    \n",
    "    Input: \n",
    "        datadir: A directory containing XML files\n",
    "        \n",
    "    Output: A list of classification cases. Each case is a list containing sentenceid, entity1_id, entity2_id,\n",
    "            ground truth relation label, and a list of sentence tokens (each token containing any needed information:\n",
    "            word, lemma, PoS, offsets, etc.\n",
    "    '''\n",
    "    dataset = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for f in listdir(datadir):\n",
    "        \n",
    "        tree = parse(datadir + \"/\" + f)\n",
    "        sentences = tree.getElementsByTagName(\"sentence\")\n",
    "\n",
    "        for s in sentences: \n",
    "            sid = s.attributes[\"id\"].value   \n",
    "            stext = s.attributes[\"text\"].value\n",
    "            stext = stext.replace(\"-\",\" \")\n",
    "            \n",
    "            ents = {}\n",
    "            entities = s.getElementsByTagName(\"entity\")\n",
    "            for e in entities:\n",
    "                e_id = e.attributes[\"id\"].value\n",
    "                offset = e.attributes[\"charOffset\"].value\n",
    "                name = e.attributes[\"text\"].value\n",
    "                ents[e_id] = name\n",
    "\n",
    "            pairs = s.getElementsByTagName(\"pair\")\n",
    "            for p in pairs:\n",
    "                pair_sent = stext\n",
    "                e1_id = p.attributes['e1'].value\n",
    "                e2_id = p.attributes['e2'].value\n",
    "                if p.attributes['ddi'].value == 'true':\n",
    "                    ddi = p.attributes['type'].value\n",
    "                else:\n",
    "                    ddi = 'null'\n",
    "                \n",
    "                aux = [sid, e1_id, e2_id, ddi]\n",
    "                for key, item in ents.items():\n",
    "                    if key == e1_id:\n",
    "                        drug_1 = item\n",
    "                        pair_sent = pair_sent.replace(drug_1, 'DRUG1')\n",
    "                    elif key == e2_id:\n",
    "                        drug_2 = item\n",
    "                        pair_sent = pair_sent.replace(drug_2, 'DRUG2')\n",
    "                    else:\n",
    "                        other_drug = item\n",
    "                        pair_sent = pair_sent.replace(other_drug, 'DRUGOTHER')\n",
    "        \n",
    "                punct = [\".\",\",\",\";\",\":\",\"?\",\"!\", \"'\"] # TODO: Change all punctuations to <PUNCT> and stopwords to <SW>\n",
    "                pair_sent = pair_sent.replace(\"(\", \"\") # Remove Parenthesis\n",
    "                pair_sent = pair_sent.replace(\")\", \"\") # Remove Parenthesis\n",
    "                tokens = word_tokenize(pair_sent)\n",
    "                aux2 = []\n",
    "                for t in tokens:\n",
    "                    if t in punct:\n",
    "                        tk = ('<PUNCT>', '<PUNCT>')\n",
    "                    elif t in stopwords:\n",
    "                        tk = ('<SW>', '<SW>')\n",
    "                    else:\n",
    "                        if t not in ['DRUG1','DRUG2', 'DRUGOTHER']:\n",
    "                            t_l = t.lower()\n",
    "                            #lemma = wordnet_lemmatizer.lemmatize(t_l)\n",
    "                            stem = stemmer.stem(t_l)\n",
    "                            tk = (t, stem) # TODO: Add PoS Tag and maybe use stemmer instead of lemmatizer\n",
    "                        else:\n",
    "                            tk = ('<' + str(t) + '>','<' + str(t) + '>')\n",
    "                    aux2.append(tk)\n",
    "                aux.append(aux2)\n",
    "                dataset.append(aux)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDI-DrugBank.d234.s2', 'DDI-DrugBank.d234.s2.e0', 'DDI-DrugBank.d234.s2.e1', 'effect', [('Particular', 'particular'), ('caution', 'caution'), ('<SW>', '<SW>'), ('necessary', 'necessari'), ('<SW>', '<SW>'), ('using', 'use'), ('<DRUG1>', '<DRUG1>'), ('<SW>', '<SW>'), ('cases', 'case'), ('<SW>', '<SW>'), ('mixed', 'mix'), ('drug', 'drug'), ('overdosage', 'overdosag'), ('since', 'sinc'), ('<SW>', '<SW>'), ('toxic', 'toxic'), ('effects', 'effect'), ('<SW>', '<SW>'), ('<SW>', '<SW>'), ('convulsions', 'convuls'), ('<SW>', '<SW>'), ('cardiac', 'cardiac'), ('dysrhythmias', 'dysrhythmia'), ('<SW>', '<SW>'), ('<SW>', '<SW>'), ('drugs', 'drug'), ('taken', 'taken'), ('<SW>', '<SW>'), ('overdose', 'overdos'), ('especially', 'especi'), ('<DRUG2>', '<DRUG2>'), ('may', 'may'), ('emerge', 'emerg'), ('<SW>', '<SW>'), ('<SW>', '<SW>'), ('reversal', 'revers'), ('<SW>', '<SW>'), ('<SW>', '<SW>'), ('<DRUGOTHER>', '<DRUGOTHER>'), ('effect', 'effect'), ('<SW>', '<SW>'), ('<DRUGOTHER>', '<DRUGOTHER>'), ('<PUNCT>', '<PUNCT>')]]\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../labAHLT/data/train'\n",
    "devel_path = '../../labAHLT/data/devel'\n",
    "dataset = load_data(train_path)\n",
    "val_dataset = load_data(devel_path)\n",
    "print(dataset[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset, max_length):\n",
    "    '''\n",
    "    Task: Create index dictionaries both for input (words) and output (labels) from given dataset\n",
    "    Input: \n",
    "        dataset: dataset produced by load_data.\n",
    "        max_length: maximum length of a sentence (longer sentences will be cut, shorter ones will be padded).\n",
    "        \n",
    "    Output: A dictionary where each key is an index name (e.g. \"words\", \"labels\"), and the value is a \n",
    "            dictionary mapping each word/label to a number. An entry with the value for maxlen is also stored\n",
    "    Example: \n",
    "        >>> create_indx(traindata)\n",
    "        {'words': {'<PAD>':0, '<UNK>':1, '11-day':2, 'murine':3, 'criteria':4,\n",
    "                   'stroke':5, ... ,'levodopa':8511, 'terfenadine': 8512}\n",
    "         'labels': {' null ':0, 'mechanism ':1, 'advise ':2, 'effect ':3, 'int ':4}\n",
    "         'maxlen': 100 }\n",
    "    '''\n",
    "    index_words = {'<PAD>':0, '<UNK>':1}\n",
    "    i = 2\n",
    "    \n",
    "    index_stems = {'<PAD>':0, '<UNK>':1}\n",
    "    z = 2\n",
    "    \n",
    "    index_labels = {}\n",
    "    j = 0\n",
    "    \n",
    "    index_suf = {'<PAD>':0, '<UNK>':1}\n",
    "    ii = 2\n",
    "    \n",
    "    index_pref = {'<PAD>':0, '<UNK>':1}\n",
    "    iii = 2\n",
    "    \n",
    "    pos_tags = ['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN',\n",
    "                'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', \n",
    "                '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD',\n",
    "                'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']\n",
    "    index_pos = {'<PAD>':0, '<UNK>':1}\n",
    "    y = 2\n",
    "    for t in pos_tags:\n",
    "        index_pos[t] = y\n",
    "        y += 1\n",
    "    \n",
    "    # We can create other indexes (lemmas, PoS, etc)  \n",
    "    \n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        label = s[3]\n",
    "        if label not in index_labels:\n",
    "            index_labels[label] = j\n",
    "            j += 1\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            suff = w[-4:]\n",
    "            pref = w[:4]\n",
    "            s = tup[1]\n",
    "            if w not in index_words:\n",
    "                index_words[w] = i\n",
    "                i += 1\n",
    "            if suff not in index_suf:\n",
    "                index_suf[suff] = ii\n",
    "                ii += 1\n",
    "            if pref not in index_pref:\n",
    "                index_pref[pref] = iii\n",
    "                iii += 1\n",
    "            if s not in index_stems:\n",
    "                index_stems[s] = z\n",
    "                z += 1\n",
    "                \n",
    "    idx = {'words': index_words, 'suffixes': index_suf, 'prefixes':index_pref, 'stems':index_stems, 'PoS':index_pos, 'labels':index_labels, 'maxlen':max_length}     \n",
    "    \n",
    "    return idx\n",
    "    \n",
    "# Add '<PAD>': 0 and '<UNK>':1 codes to 'words' index. The coding of the rest of the words/labels is arbitrary.\n",
    "# You may add to the dictionary entries with indexes for other elements you want to use (lemmas, PoS, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, 'LS': 2, 'TO': 3, 'VBN': 4, \"''\": 5, 'WP': 6, 'UH': 7, 'VBG': 8, 'JJ': 9, 'VBZ': 10, '--': 11, 'VBP': 12, 'NN': 13, 'DT': 14, 'PRP': 15, ':': 16, 'WP$': 17, 'NNPS': 18, 'PRP$': 19, 'WDT': 20, '(': 21, ')': 22, '.': 23, ',': 24, '``': 25, '$': 26, 'RB': 27, 'RBR': 28, 'RBS': 29, 'VBD': 30, 'IN': 31, 'FW': 32, 'RP': 33, 'JJR': 34, 'JJS': 35, 'PDT': 36, 'MD': 37, 'VB': 38, 'WRB': 39, 'NNP': 40, 'EX': 41, 'NNS': 42, 'SYM': 43, 'CC': 44, 'CD': 45, 'POS': 46}\n"
     ]
    }
   ],
   "source": [
    "idx = create_index(dataset, 100)\n",
    "#print(idx[\"words\"])\n",
    "#print(idx[\"labels\"])\n",
    "#print(idx['maxlen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the words in a sentence dataset formed by lists of tokens into lists of indexes\n",
    "          suitable for NN input.\n",
    "    Input: \n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, as well\n",
    "             as the maximum sentence length.\n",
    "             \n",
    "    Output: The dataset encoded as a list of sentence, each of them is a list of word indices.\n",
    "            If the word is not in the index, <UNK> code is used. If the sentence is shorter than\n",
    "            max_len it is padded with <PAD> code.\n",
    "    Example: \n",
    "        >>> encode_words(traindata, idx)\n",
    "            [ [6882 1049 4911 ... 0 0 0 ]\n",
    "            [  2290 7548 8069 ... 0 0 0 ]\n",
    "               ...\n",
    "            [  2002 6582 7518 ... 0 0 0 ] ]\n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        aux = []\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            if w in idx['words']:\n",
    "                i = idx['words'][w]\n",
    "            else:\n",
    "                i = idx['words']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "        \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "\n",
    "def encode_suffixes(dataset, idx):\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        aux = []\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            suff = w[-4:]\n",
    "            if suff in idx['suffixes']:\n",
    "                i = idx['suffixes'][suff]\n",
    "            else:\n",
    "                i = idx['suffixes']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "        \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "\n",
    "def encode_prefixes(dataset, idx):\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        aux = []\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            pref = w[:4]\n",
    "            if pref in idx['prefixes']:\n",
    "                i = idx['prefixes'][pref]\n",
    "            else:\n",
    "                i = idx['prefixes']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "        \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "\n",
    "def encode_stems(dataset, idx):\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        aux = []\n",
    "        for tup in words:\n",
    "            w = tup[1]\n",
    "            if w in idx['stems']:\n",
    "                i = idx['stems'][w]\n",
    "            else:\n",
    "                i = idx['stems']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "        \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "        \n",
    "def encode_tags(dataset, idx):\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    for s in dataset:\n",
    "        words = s[4]\n",
    "        aux = []\n",
    "        for tup in words:\n",
    "            w = tup[0]\n",
    "            pos = nltk.pos_tag(w)\n",
    "            if w in idx['PoS']:\n",
    "                i = idx['PoS'][w]\n",
    "            else:\n",
    "                i = idx['PoS']['<UNK>']\n",
    "            aux.append(i)\n",
    "        seq.append(aux)\n",
    "        \n",
    "    seq_padded = pad_sequences(maxlen = max_length, sequences = seq, padding = 'post')\n",
    "    \n",
    "    return seq_padded\n",
    "        \n",
    "def encode_labels(dataset, idx):\n",
    "    '''\n",
    "    Task: Encode the ground truth labels in a dataset of classification examples (sentence + entity pair).\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data.\n",
    "        idx: A dictionary produced by create_index, containing word and label indexes, as well as the maximum length.\n",
    "        \n",
    "    Output: The dataset encoded as a list DDI labels, one per classification example. \n",
    "    \n",
    "    Example :\n",
    "     >>> encode_labels ( traindata , idx )\n",
    "     [  [0] [0] [2] ... [4] [0] [0] [1] [0] ]\n",
    "     [  [4] [6] [4] [4] [4] [4] ... [0] [0] ]\n",
    "     [  [4] [4] [8] [4] [6] [4] ... [0] [0] ]\n",
    "         ...\n",
    "     [  [4] [8] [9] [4] [4] [4] ... [0] [0] ]\n",
    "     ] \n",
    "    '''\n",
    "    max_length = idx['maxlen']\n",
    "    seq = []\n",
    "    seq = [idx['labels'][s[3]] for s in dataset] # Changed [[idx['labels'][s[3]]] for s in dataset]\n",
    "    Y = [to_categorical(i, num_classes=5) for i in seq]\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    3    4 ...    0    0    0]\n",
      " [  15   16   17 ...    0    0    0]\n",
      " [  15   16   17 ...    0    0    0]\n",
      " ...\n",
      " [1198  996    6 ...    0    0    0]\n",
      " [1198  996    6 ...    0    0    0]\n",
      " [1198  996    6 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = encode_words(dataset, idx)\n",
    "X_train_suff = encode_suffixes(dataset, idx)\n",
    "X_train_pref = encode_prefixes(dataset, idx)\n",
    "X_train_stems = encode_stems(dataset, idx)\n",
    "\n",
    "X_val = encode_words(val_dataset, idx)\n",
    "X_val_suff = encode_suffixes(val_dataset, idx)\n",
    "X_val_pref = encode_prefixes(val_dataset, idx)\n",
    "X_val_stems = encode_stems(val_dataset, idx)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pos = encode_tags(dataset, idx)\n",
    "X_val_pos = encode_tags(dataset, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23148, 5)\n",
      "(4669, 5)\n"
     ]
    }
   ],
   "source": [
    "Y_train = encode_labels(dataset, idx)\n",
    "Y_val = encode_labels(val_dataset, idx)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GloVe\n",
    "# http://nlp.stanford.edu/data/glove.6B.zip\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index['words']) + 1  \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index['words']:\n",
    "                idx = word_index['words'][word] \n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"../../labAHLT/data/glove.6B/glove.6B.50d.txt\"\n",
    "embedding_matrix = create_embedding_matrix(glove_path, idx, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, embedding_dim, embedding_matrix):\n",
    "\n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # create network layers\n",
    "    inp = Input(shape=(max_len,))\n",
    "    emb_layer = Embedding(input_dim=n_words+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len)(inp) # TransferLearning -> GloVe, BERT\n",
    "    model = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.4))(emb_layer)\n",
    "    #model = Conv1D(64, 5, activation='relu')(model)\n",
    "    model = GlobalMaxPooling1D()(model)\n",
    "    model = Dense(24, activation='relu')(model)\n",
    "    out = Dense(n_labels, activation='softmax')(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, embedding_dim, embedding_matrix):\n",
    "    '''\n",
    "    Task: Create network for the learner. \n",
    "    Input:\n",
    "        idx: index dictionary with word/labels codes, plus maximum sentence length.\n",
    "    Output: Returns a compiled Keras neural network with the specified layers\n",
    "    '''\n",
    "    \n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    n_suf = len(idx['suffixes'])\n",
    "    n_pref = len(idx['prefixes'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # Input layers\n",
    "    inp = Input(shape=(max_len,), name=\"words\")\n",
    "    inp_suf = Input(shape=(max_len,), name=\"suf\")\n",
    "    inp_pref = Input(shape=(max_len,), name=\"pref\")\n",
    "\n",
    "    # Embedding layers\n",
    "    emb_words = Embedding(input_dim=n_words+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len)(inp)\n",
    "    emb_suffs = Embedding(input_dim =n_suf+1, output_dim=64, input_length=max_len)(inp_suf)\n",
    "    emb_prefs = Embedding(input_dim =n_pref+1, output_dim=64, input_length=max_len)(inp_pref)\n",
    "\n",
    "    # LSTM Layers\n",
    "    model1 = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.4, dropout=0.3))(emb_words)\n",
    "    model2 = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(emb_suffs)\n",
    "    model3 = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(emb_prefs)\n",
    "\n",
    "    model = Concatenate()([model1, model2, model3])\n",
    "    model = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.4, dropout=0.2))(model)\n",
    "    \n",
    "    model = GlobalMaxPooling1D()(model)\n",
    "    model = Dense(32, activation='relu')(model)\n",
    "    out = Dense(n_labels, activation='softmax')(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inputs=[inp, inp_suf, inp_pref], outputs=out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, embedding_dim, embedding_matrix):\n",
    "\n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    n_suf = len(idx['suffixes'])\n",
    "    n_pref = len(idx['prefixes'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # Input layers\n",
    "    inp = Input(shape=(max_len,), name=\"words\")\n",
    "    inp_suf = Input(shape=(max_len,), name=\"suf\")\n",
    "    inp_pref = Input(shape=(max_len,), name=\"pref\")\n",
    "\n",
    "    # Embedding layers\n",
    "    emb_words = Embedding(input_dim=n_words+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len)(inp)\n",
    "    emb_suffs = Embedding(input_dim =n_suf+1, output_dim=64, input_length=max_len)(inp_suf)\n",
    "    emb_prefs = Embedding(input_dim =n_pref+1, output_dim=64, input_length=max_len)(inp_pref)\n",
    "    \n",
    "    \n",
    "    model1 = Conv1D(128, 5, activation='relu')(emb_words)\n",
    "    model1 = GlobalMaxPooling1D()(model1)\n",
    "    model2 = Conv1D(128, 5, activation='relu')(emb_suffs)\n",
    "    model2 = GlobalMaxPooling1D()(model2)\n",
    "    model3 = Conv1D(128, 5, activation='relu')(emb_prefs)\n",
    "    model3 = GlobalMaxPooling1D()(model3)\n",
    "    \n",
    "    \n",
    "    model = Concatenate()([model1, model2, model3])\n",
    "    model4 = Dense(64, activation='relu')(model)\n",
    "    out = Dense(n_labels, activation='softmax')(model4)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inputs=[inp, inp_suf, inp_pref], outputs=out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, embedding_dim, embedding_matrix):\n",
    "   #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    n_suf = len(idx['suffixes'])\n",
    "    n_pref = len(idx['prefixes'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # Input layers\n",
    "    inp = Input(shape=(max_len,), name=\"words\")\n",
    "    inp_suf = Input(shape=(max_len,), name=\"suf\")\n",
    "    inp_pref = Input(shape=(max_len,), name=\"pref\")\n",
    "\n",
    "    # Embedding layers\n",
    "    emb_words = Embedding(input_dim=n_words+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len)(inp)\n",
    "    emb_suffs = Embedding(input_dim =n_suf+1, output_dim=64, input_length=max_len)(inp_suf)\n",
    "    emb_prefs = Embedding(input_dim =n_pref+1, output_dim=64, input_length=max_len)(inp_pref)\n",
    "\n",
    "    # LSTM Layers\n",
    "    model1 = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.4, dropout=0.3))(emb_words)\n",
    "    model2 = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(emb_suffs)\n",
    "    model3 = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(emb_prefs)\n",
    "    model = Concatenate()([model1, model2, model3])\n",
    "\n",
    "    model = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.4))(model)\n",
    "    model = Conv1D(128, 5, activation='relu')(model)\n",
    "    model = GlobalMaxPooling1D()(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "    #model = Dense(24, activation='relu')(model)\n",
    "    out = Dense(n_labels, activation='softmax')(model)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inputs=[inp, inp_suf, inp_pref], outputs=out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.005, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, embedding_dim, embedding_matrix):\n",
    "\n",
    "    #sizes\n",
    "    n_words = len(idx['words'])\n",
    "    n_labels = len(idx['labels'])\n",
    "    n_suf = len(idx['suffixes'])\n",
    "    n_pref = len(idx['prefixes'])\n",
    "    max_len = idx['maxlen']\n",
    "    \n",
    "    # Input layers\n",
    "    inp = Input(shape=(max_len,), name=\"words\")\n",
    "    inp_suf = Input(shape=(max_len,), name=\"suf\")\n",
    "    inp_pref = Input(shape=(max_len,), name=\"pref\")\n",
    "\n",
    "    # Embedding layers\n",
    "    emb_words = Embedding(input_dim=n_words+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len)(inp)\n",
    "    emb_suffs = Embedding(input_dim =n_suf+1, output_dim=64, input_length=max_len)(inp_suf)\n",
    "    emb_prefs = Embedding(input_dim =n_pref+1, output_dim=64, input_length=max_len)(inp_pref)\n",
    "    \n",
    "    convs = []\n",
    "    convs1 = []\n",
    "    convs2 = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200,  kernel_size=filter_size, activation='relu')(emb_words)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "                \n",
    "        r_conv = Conv1D(filters=200,  kernel_size=filter_size, activation='relu')(emb_suffs)\n",
    "        r_pool = GlobalMaxPooling1D()(r_conv)\n",
    "        convs1.append(r_pool)\n",
    "        \n",
    "        t_conv = Conv1D(filters=200,  kernel_size=filter_size, activation='relu')(emb_prefs)\n",
    "        t_pool = GlobalMaxPooling1D()(t_conv)\n",
    "        convs2.append(t_pool)\n",
    "        \n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    r_merge = concatenate(convs1, axis=1)\n",
    "    t_merge = concatenate(convs2, axis=1)\n",
    "    x1 = Dropout(0.1)(l_merge)\n",
    "    x2 = Dropout(0.1)(r_merge) \n",
    "    x3 = Dropout(0.1)(t_merge) \n",
    "    model = Concatenate()([x1, x2, x3])\n",
    "\n",
    "    x = Dense(128, activation='relu')(model)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(n_labels, activation='softmax')(x)\n",
    "    \n",
    "    # create and compile model\n",
    "    model = Model(inputs=[inp, inp_suf, inp_pref], outputs=out)\n",
    "    \n",
    "    optimiz = Adam(lr=0.01, decay=1e-6)\n",
    "    model.compile(optimizer=optimiz, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words (InputLayer)              [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "suf (InputLayer)                [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pref (InputLayer)               [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_53 (Embedding)        (None, 100, 16)      79760       words[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_54 (Embedding)        (None, 100, 64)      160704      suf[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_55 (Embedding)        (None, 100, 64)      180544      pref[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_42 (Bidirectional (None, 100, 64)      12544       embedding_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_43 (Bidirectional (None, 100, 64)      24832       embedding_54[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_44 (Bidirectional (None, 100, 64)      24832       embedding_55[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 100, 192)     0           bidirectional_42[0][0]           \n",
      "                                                                 bidirectional_43[0][0]           \n",
      "                                                                 bidirectional_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_45 (Bidirectional (None, 100, 128)     131584      concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 96, 128)      82048       bidirectional_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_69 (Global (None, 128)          0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           global_max_pooling1d_69[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 5)            645         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 697,493\n",
      "Trainable params: 697,493\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_network(idx, 16, embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "724/724 [==============================] - 222s 288ms/step - loss: 0.5230 - accuracy: 0.8551 - val_loss: 0.3922 - val_accuracy: 0.8636\n",
      "Epoch 2/5\n",
      "724/724 [==============================] - 217s 299ms/step - loss: 0.2692 - accuracy: 0.9004 - val_loss: 0.3896 - val_accuracy: 0.8646\n",
      "Epoch 3/5\n",
      "724/724 [==============================] - 222s 307ms/step - loss: 0.1748 - accuracy: 0.9366 - val_loss: 0.4956 - val_accuracy: 0.8486\n",
      "Epoch 4/5\n",
      "724/724 [==============================] - 237s 328ms/step - loss: 0.1394 - accuracy: 0.9490 - val_loss: 0.4375 - val_accuracy: 0.8702\n",
      "Epoch 5/5\n",
      "724/724 [==============================] - 241s 333ms/step - loss: 0.1194 - accuracy: 0.9586 - val_loss: 0.4640 - val_accuracy: 0.8738\n"
     ]
    }
   ],
   "source": [
    "history = model.fit({\"words\":X_train, \"suf\":X_train_suff, \"pref\":X_train_pref}, Y_train,\n",
    "                   batch_size=32, #16/64\n",
    "                   epochs=5,\n",
    "                   verbose=1, \n",
    "                   validation_data=({\"words\":X_val, \"suf\":X_val_suff, \"pref\":X_val_pref}, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_indexes(model, idx):\n",
    "    '''\n",
    "    Task: Save given model and indexs to disk\n",
    "    Input: \n",
    "        model: Keras model created by _build_network, and trained.\n",
    "        idx: A dictionary produced by create_indexs, containing word and label indexes, \n",
    "             as well as the maximum sentence length. \n",
    "        filename: Saves the mode into filename.nn and the indexes into filename.idx\n",
    "    '''\n",
    "    model.save(\"ddi-nn.nn\")\n",
    "    \n",
    "    file = open(\"index.pkl\", \"wb\")\n",
    "    pickle.dump(idx, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ddi-nn.nn/assets\n"
     ]
    }
   ],
   "source": [
    "save_model_and_indexes(model, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner(traindir, validationdir):\n",
    "    '''\n",
    "    Learns a NN model using traindir as training data, and validationdir as validation data.\n",
    "    Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    traindata = load_data(traindir)\n",
    "    valdata = load_data(validationdir)\n",
    "    \n",
    "    # create indexes from trainindg data \n",
    "    max_len = 100 # reducing it seems to decrease accuracy\n",
    "    idx = create_index(traindata, max_len)\n",
    "    embedding_matrix = create_embedding_matrix(glove_path, idx, 16)\n",
    "    \n",
    "    # build network \n",
    "    model = build_network(idx, 16, embedding_matrix)\n",
    "    \n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(traindata, idx)\n",
    "    Ytrain = encode_labels(traindata, idx)\n",
    "    Xval = encode_words(valdata, idx)\n",
    "    Yval = encode_labels(valdata, idx)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(Xtrain, Ytrain,\n",
    "              batch_size=16,\n",
    "              verbose=1,\n",
    "              epochs=3,\n",
    "              validation_data=(Xval, Yval))\n",
    "    \n",
    "    #return history\n",
    "    \n",
    "    #save model and indexs, for later use in prediction\n",
    "    save_model_and_indexes(model, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1447/1447 [==============================] - 17s 11ms/step - loss: 0.6862 - accuracy: 0.8396 - val_loss: 0.5897 - val_accuracy: 0.8546\n",
      "Epoch 2/3\n",
      "1447/1447 [==============================] - 17s 12ms/step - loss: 0.4496 - accuracy: 0.8591 - val_loss: 0.4865 - val_accuracy: 0.8518\n",
      "Epoch 3/3\n",
      "1447/1447 [==============================] - 17s 12ms/step - loss: 0.4303 - accuracy: 0.8716 - val_loss: 0.5551 - val_accuracy: 0.8496\n",
      "INFO:tensorflow:Assets written to: ddi-nn.nn/assets\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../labAHLT/data/train'\n",
    "devel_path = '../../labAHLT/data/devel'\n",
    "\n",
    "learner(train_path, devel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')\n",
    "# Need more epochs but it seems to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_indexs():\n",
    "    '''\n",
    "    Task: Load model and associate indexs from disk.\n",
    "    Input:\n",
    "        filename: filename to be loaded\n",
    "    Output: Loads a model from filename.nn and its indexes from filename.idx\n",
    "            Returns the loaded model and indexes. \n",
    "    '''\n",
    "    \n",
    "    model = load_model(\"ddi-nn.nn\")\n",
    "    index = open(\"index.pkl\", \"rb\")\n",
    "    idx = pickle.load(index)\n",
    "    \n",
    "    return model, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_interactions(dataset, preds, outf): \n",
    "    '''\n",
    "    Task: Output detected DDIs in the format expected by the evaluator.\n",
    "    Input:\n",
    "        dataset: A dataset produced by load_data\n",
    "        preds: Fore each sentence in dataset, a label for its DDI type (or 'null' if no DDI detected)\n",
    "        \n",
    "    Output: prints the detected interactions to dtdout in the format required by the evaluator.\n",
    "    Example:\n",
    "        >>> output_interactions ( dataset , preds )\n",
    "            DDI-DrugBank.d398.s0|DDI-DrugBank.d398.s0.e0|DDI-DrugBank.d398.s0.e1|effect\n",
    "            DDI-DrugBank.d398.s0|DDI-DrugBank.d398.s0.e0|DDI-DrugBank.d398.s0.e2|effect\n",
    "            DDI-DrugBank.d211.s2|DDI-DrugBank.d211.s2.e0|DDI-DrugBank.d211.s2.e5|mechanism\n",
    "            ...\n",
    "    '''\n",
    "    length = len(dataset)\n",
    "    for i in range(length):\n",
    "        sid = dataset[i][0]\n",
    "        id_e1 = dataset[i][1]\n",
    "        id_e2 = dataset[i][2]\n",
    "        ddi_type = preds[i]\n",
    "        outf.write(str(sid) +\"|\"+ str(id_e1) +\"|\"+ str(id_e2) +\"|\"+ str(ddi_type))\n",
    "        outf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Classifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(datadir, outfile):\n",
    "    '''\n",
    "    Loads a NN model from a file 'modelname' and uses it to extract drugs in datadir. Saves\n",
    "    results to 'outfile' in the appropriate format\n",
    "    '''\n",
    "    \n",
    "    # load model and associated encoding data\n",
    "    #model, idx = load_model_and_indexs()\n",
    "    \n",
    "    # load data to annotate\n",
    "    testdata = load_data(datadir)\n",
    "    \n",
    "    # encode dataset\n",
    "    X = encode_words(testdata, idx)\n",
    "    X_suff = encode_suffixes(testdata, idx)\n",
    "    X_pref = encode_prefixes(testdata, idx)\n",
    "    \n",
    "    # tag sentences in dataset\n",
    "    Y = model.predict({\"words\": X, \"suf\": X_suff, \"pref\": X_pref})\n",
    "    # get most likely tag for each word\n",
    "    preds = []\n",
    "    for s in Y:\n",
    "        it = np.argmax(s)\n",
    "        for key, item in idx['labels'].items():\n",
    "            if item == it:\n",
    "                preds.append(key)\n",
    "    #Y = [[idx['labels'][np.argmax(y)] for y in s] for s in Y] # np.argmax(y) is not a key\n",
    "    \n",
    "    # extract entities and dump them to output file\n",
    "    outf = open(outfile, \"w\")\n",
    "    output_interactions(testdata, preds, outf)\n",
    "    outf.close()\n",
    "    \n",
    "    # evaluate using official evaluator\n",
    "    evaluate(\"DDI\", datadir, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
      "------------------------------------------------------------------------------\n",
      "advise             68\t  56\t  70\t 124\t 138\t54.8%\t49.3%\t51.9%\n",
      "effect            189\t 117\t 126\t 306\t 315\t61.8%\t60.0%\t60.9%\n",
      "int                17\t   3\t  18\t  20\t  35\t85.0%\t48.6%\t61.8%\n",
      "mechanism         123\t  83\t 141\t 206\t 264\t59.7%\t46.6%\t52.3%\n",
      "------------------------------------------------------------------------------\n",
      "M.avg            -\t-\t-\t-\t-\t65.3%\t51.1%\t56.7%\n",
      "------------------------------------------------------------------------------\n",
      "m.avg             397\t4272\t 355\t4669\t 752\t8.5%\t52.8%\t14.6%\n",
      "m.avg(no class)   752\t3917\t   0\t4669\t 752\t16.1%\t100.0%\t27.7%\n"
     ]
    }
   ],
   "source": [
    "devel_path = '../../labAHLT/data/devel'\n",
    "predict(devel_path, 'val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   tp\t  fp\t  fn\t#pred\t#exp\tP\tR\tF1\n",
      "------------------------------------------------------------------------------\n",
      "advise             97\t  78\t 115\t 175\t 212\t55.4%\t45.8%\t50.1%\n",
      "effect            145\t  77\t 138\t 222\t 283\t65.3%\t51.2%\t57.4%\n",
      "int                 1\t   7\t  17\t   8\t  18\t12.5%\t5.6%\t7.7%\n",
      "mechanism         161\t 103\t 176\t 264\t 337\t61.0%\t47.8%\t53.6%\n",
      "------------------------------------------------------------------------------\n",
      "M.avg            -\t-\t-\t-\t-\t48.6%\t37.6%\t42.2%\n",
      "------------------------------------------------------------------------------\n",
      "m.avg             404\t5287\t 446\t5691\t 850\t7.1%\t47.5%\t12.4%\n",
      "m.avg(no class)   850\t4841\t   0\t5691\t 850\t14.9%\t100.0%\t26.0%\n"
     ]
    }
   ],
   "source": [
    "test_path = '../../labAHLT/data/test'\n",
    "predict(test_path, 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Sentence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_path = '../../labAHLT/data/train'\n",
    "sent_lst = []\n",
    "\n",
    "for f in listdir(train_path):\n",
    "    tree = parse(train_path + \"/\" + f)\n",
    "    sentences = tree.getElementsByTagName(\"sentence\")    \n",
    "    \n",
    "    for s in sentences: \n",
    "        stext = s.attributes[\"text\"].value\n",
    "        sent_lst.append(stext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'sentence': sent_lst}\n",
    "sent_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milk, milk products, and calcium-rich foods or...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interactions between Betaseron and other drugs...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although studies designed to examine drug inte...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Betaseron administration to three cancer patie...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Differential regulation of tyrosine phosphoryl...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5405</th>\n",
       "      <td>Lovastatin therapy has not been associated wit...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>Propranolol: In normal volunteers, there was n...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>Digoxin: In patients with hypercholesterolemia...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>Oral Hypoglycemic Agents: In pharmacokinetic s...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>May interact with addictive medications, espec...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5410 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  word_count\n",
       "0     Milk, milk products, and calcium-rich foods or...          14\n",
       "1     Interactions between Betaseron and other drugs...          11\n",
       "2     Although studies designed to examine drug inte...          36\n",
       "3     Betaseron administration to three cancer patie...          42\n",
       "4     Differential regulation of tyrosine phosphoryl...          19\n",
       "...                                                 ...         ...\n",
       "5405  Lovastatin therapy has not been associated wit...          19\n",
       "5406  Propranolol: In normal volunteers, there was n...          23\n",
       "5407  Digoxin: In patients with hypercholesterolemia...          19\n",
       "5408  Oral Hypoglycemic Agents: In pharmacokinetic s...          24\n",
       "5409  May interact with addictive medications, espec...          43\n",
       "\n",
       "[5410 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(s):\n",
    "    words = s.split()\n",
    "    return len(words)\n",
    "\n",
    "sent_df['word_count'] = sent_df['sentence'].apply(count_words)\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(round(sent_df['word_count'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sent_df['word_count'].hist(figsize = (8,5), legend = False, color='darkgrey', edgecolor='dimgray')\n",
    "ax.yaxis.grid(color='darkgrey', linestyle=':', linewidth=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
